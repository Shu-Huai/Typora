# 数据相关

## 3D 数据的表示方法

### 传统方法

#### 点云

由一组三维点表示物体或场景的数据结构

每个点具有空间坐标 $(x,y,z)$ 和一些其他的属性，如颜色等

大多来自深度相机或激光扫描仪、激光雷达等捕获的原始数据

<img src="http://public.file.lvshuhuai.cn/images\image-20241121221457538.png" alt="image-20241121221457538" style="zoom:50%;" />

#### 网格

网格通过顶点（Vertices）、边（Edges）和面（Faces）表示 3D 对象的几何结构。常见的网格类型包括三角网格和四边形网格。

提供额外的几何细节、编码拓扑并包含表面法线信息

<img src="http://public.file.lvshuhuai.cn/images\image-20241121221310256.png" alt="image-20241121221310256" style="zoom:50%;" />

#### 体素

三维空间的规则网格

类似于像素，只不过是三维的

每个单元格表示一个均匀区域，类似二维图像的像素

<img src="http://public.file.lvshuhuai.cn/images\image-20241121221425578.png" alt="image-20241121221425578" style="zoom:50%;" />

### 新方法

#### 神经网络

辐射场的方式

输入是 3D 位置，输出是颜色值和法线向量，颜色表示给定位置处的表面颜色，法线向量表示表面的方向。

<img src="http://public.file.lvshuhuai.cn/images\image-20241121221852002.png" alt="image-20241121221852002" style="zoom:50%;" />

#### 高斯团

利用高斯分布函数的参数作为基本单元来表示 3D 数据

位置（position）：高斯团的三维坐标

比例（scale）：高斯团的大小

不透明度（opacity）：高斯团对渲染图像的影响程度

颜色（color）：高斯团的颜色。

材质属性（material properties）：其他属性，例如光泽度、反射和折射。

<img src="http://public.file.lvshuhuai.cn/images\image-20241121221902765.png" alt="image-20241121221902765" style="zoom:50%;" />

## 数据集

原始的Gaussian泼溅使用了三个数据集

1. Mip-NeRF 360 数据集
2. Tanks&Temples 数据集
3. Deep Blending 数据集

### Mip-NeRF 360 数据集

由 Google 在 CVPR 2022 上提出。

Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields

HomePage：[mip-NeRF 360](https://jonbarron.info/mipnerf360/)

该数据集由 9 个场景组成，其中 5 个室外场景和 4 个室内场景，每个场景都包含一个复杂的中心对象或具有详细背景的区域。

包含counter、room、bonsai、stump、kitchen、bicycle、garden、flowers、treehill

总共有大约 16G 数据

![_DSC8683](http://public.file.lvshuhuai.cn/images/_DSC8683.JPG)

### Tanks&Temples 数据集

HomePage：[Tanks and Temples Benchmark](https://www.tanksandtemples.org/)

由英特尔实验室在 2017 年发布，Tanks and Temples: Benchmarking Large-Scale Scene Reconstruction

提供视频、图像

提供训练数据和测试数据两类，其中测试数据分为中级组和高级组，训练集提供 7 个场景的 7 个高分辨率视频，测试机提供 14 个场景的 14 个高分辨率视频。

测试数据的中级组包含雕塑、大型车辆和具有外观相机轨迹的房屋建筑，高级组则包含从内部和大型室外场景拍摄的室内场景

提供点云形式的数据作为 ground truth

![000017](http://public.file.lvshuhuai.cn/images/000017.jpg)

<img src="http://public.file.lvshuhuai.cn/images\image-20241121221057293.png" alt="image-20241121221057293" style="zoom:50%;" />

### Deep Blending 数据集

在 SIGGRAPH 2018 发布，Deep Blending for Free-Viewpoint Image-Based Rendering

HomePage：[Deep Blending for Free-Viewpoint Image-Based-Rendering](http://visual.cs.ucl.ac.uk/pubs/deepblending/)

![_MG_9917](http://public.file.lvshuhuai.cn/images/_MG_9917.jpg)

<img src="http://public.file.lvshuhuai.cn/images\image-20241121213342549.png" alt="image-20241121213342549" style="zoom:50%;" />

### ScanNet

ScanNet是一个大型真实RGB-D多模式数据集，包含超过250万张室内场景图片，1500个场景，带有相应的相机位姿、mesh模型、语义标签、实例标签和CAD模型。深度图以640×480分辨率拍摄，RGB图像以1296×968分辨率拍摄。3D模型由BundleFusion重建得到。

使用该数据集需要向作者申请，然后会收到一个python脚本，按照仓库中的README提示做就可以了。

### Nerf Synthetic 数据集

人工合成的多视角数据集

包括八个场景，分别是ship、mic、materials、lego、hotdog、ficus、drums、chairs

每个场景包含100张训练集，100张验证集和200张测试集，及对应的位姿转换矩阵，图像分辨率为800x800

![r_31](http://public.file.lvshuhuai.cn/images\r_31.png)

```json
{
    "file_path": "./train/r_31",
    "rotation": 0.012566370614359171,
    "transform_matrix": [
        [
            0.9962546825408936,
            0.025986727327108383,
            -0.08247014880180359,
            -0.3324478268623352
        ],
        [
            -0.08646754920482635,
            0.29941171407699585,
            -0.9501977562904358,
            -3.8303699493408203
        ],
        [
            0.0,
            0.9537699222564697,
            0.3005373179912567,
            1.2115048170089722
        ],
        [
            0.0,
            0.0,
            0.0,
            1.0
        ]
    ]
},
```

### 其他

[Nerf系列数据集记录_nerf数据集-CSDN博客](https://blog.csdn.net/toro180/article/details/129973285)

## 评估指标

### 分类

#### 全参考质量评估指标、无参考质量评估指标和限参考质量评估指标

全参考质量评估指标（Full-Reference Quality Assessment Metrics）是指在图像或视频质量评价中，评估算法需要参考完整的原始图像或视频（即“无失真版本”）来衡量处理后图像或视频的质量。它们通过比较原始和处理后内容之间的差异，得出一个数值化的质量评分。

无参考质量评估指标（No-Reference Quality Assessment Metrics）是在图像或视频质量评估中，无需参考原始图像或视频的“无失真版本”，仅依赖于输入的退化图像或视频本身来预测其质量。这类指标在无法获取参考数据的实际应用中非常重要，例如实时视频传输、在线图像服务和监控系统。

限参考评估指标（Reduced-Reference Quality Assessment Metrics）只需要部分参考信息，例如统计特征，而不是完整的参考图像。

#### 按计算方式分

像素差异类、感知模型类、信息论类

### 峰值信噪比 PSNR（Peak Signal-to-Noise Ratio）

常用于衡量图像处理质量

通过比较原始图像（通常被认为是“真实值”）和处理后的图像（重建或退化后的图像）之间的相似度，评估图像的质量。

#### 公式

$$\text{PSNR}=10\cdot\log10(\frac{\text{MAX}_I^2}{\text{MSE}})$$

其中：

- $\text{MAX}_I$ 表示图像像素值的最大可能值（例如，对于 8 位图像为 255）。

- $\text{MSE}$（均方误差）表示原始图像和处理后图像之间的平均像素差平方，定义为：
    $\text{MSE} = \frac{1}{{m\cdot n}} \sum_{i=0}^{m-1} \sum_{j=0}^{n-1}[I(i,j)-K(i,j)]^2$

    其中：

    - $I(i,j)$​ 是原始图像的像素值，
    - $K(i, j)$ 是处理后图像的像素值，
    - $m\times n$是图像的总像素数。

#### 含义

1. **PSNR 的单位**：以分贝（dB）为单位。
2. **高 PSNR 值**：表示两幅图像之间的差异较小，即图像质量较好。
3. **低 PSNR 值**：表示两幅图像之间的差异较大，即图像质量较差。

### 结构相似性指数 SSIM（Structural Similarity Index Measure）

用于衡量两幅图像之间结构相似性的指标

与 PSNR 主要基于像素差异不同，SSIM 更加注重图像的感知质量，能够更好地反映人类视觉系统的感知特性。

#### 公式

$$\text{SSIM}(x, y) = \frac{{(2\mu_x\mu_y + C_1)(2\sigma_{xy} + C_2)}}{{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}}$$

其中：

- $x$ 和 $y$：表示两幅图像的对应窗口。
- $\mu_x,\mu_y$：分别是 $x$ 和 $y$ 的均值，反映亮度信息。
- $\sigma_x,\sigma_y$：分别是 $x$ 和 $y$ 的标准差，反映对比度信息。
- $\sigma_{xy}$：$x$ 和 $y$ 的协方差，反映结构信息。
- $C_1,C_2$：两个小常数，用于避免分母为零，通常取：
    $C_1=(K_1 L)^2,\quad C_2 = (K_2 L)^2$
    - $L$：表示像素值的动态范围（如 8 位图像时$L=255$）。
    - $K_1, K_2$：默认值通常分别为 0.01 和 0.03。

#### 分量解读

SSIM 将图像质量分解为三个主要分量：

1. **亮度比较（Luminance comparison）**

    $l(x, y) = \frac{{2\mu_x\mu_y + C_1}}{{\mu_x^2 + \mu_y^2 + C_1}}$

    衡量两图像亮度的相似程度。

2. **对比度比较（Contrast comparison）**

    $c(x, y) = \frac{{2\sigma_x\sigma_y + C_2}}{{\sigma_x^2 + \sigma_y^2 + C_2}}$

    衡量两图像对比度的相似程度。

3. **结构比较（Structure comparison）**

    $s(x, y) = \frac{{\sigma_{xy} + C_3}}{{\sigma_x\sigma_y + C_3}}, \quad C_3 = C_2 / 2$

    衡量两图像局部结构的相似程度。

SSIM 是这三部分的组合：

$\text{SSIM}(x, y) = [l(x, y)]^\alpha \cdot [c(x, y)]^\beta \cdot [s(x, y)]^\gamma$

一般情况下，$\alpha = \beta = \gamma = 1$

#### 范围

$[-1,1]$

- 1表示完全相同。
- 0表示没有相似性。
- 负值表示可能表示图像的某种失真。

### 学习感知图像块相似性 LPIPS（Learned Perceptual Image Patch Similarity）

用来衡量图像感知相似性的无参考质量评估指标

通过预训练的深度神经网络提取特征，评估图像块之间的感知差异，旨在更贴近人类视觉系统的感知特性

更关注感知层面的差异，对图像高频细节（如纹理、结构）的变化更加敏感

#### LPIPS 的核心思想

1. **深度特征提取**：使用预训练的卷积神经网络（如 VGG、AlexNet 等）从图像中提取深层次的特征。
2. **特征空间对比**：将两幅图像的特征图进行逐层比较，计算每一层特征之间的感知差异。
3. **感知加权**：通过学习的权重对每层特征差异进行加权，得到一个综合的感知相似度分数。

#### 公式

给定两幅图像 $x$ 和 $x_0$：

1. 使用预训练的神经网络提取每一层的特征表示： $F^l(x),F^l(x_0)$ 其中 $l$ 表示网络的第 $l$ 层。

2. 计算每一层特征图的欧几里得距离（L2 范数）：

    $d^l(x,x_0)=\|w^l\odot(F^l(x)-F^l(x_0))\|_2$

    - $w^l$：每层的感知加权系数（可通过训练优化）。
    - $\odot$：逐点乘法。

3. 对所有层的距离进行加权求和，得到最终的感知相似性分数：$\text{LPIPS}(x, x_0) = \sum_l d^l(x, x_0)$​

#### 含义

- **LPIPS 越小**：两幅图像的感知相似性越高。
- **LPIPS 越大**：两幅图像在感知上差异越明显。
