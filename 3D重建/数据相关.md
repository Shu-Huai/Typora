# 数据相关

## 3D数据的表示方法

### 传统方法

#### 点云

#### 网格

#### 体素

### 新方法

#### 高斯团



## 数据集

原始的Gaussian泼溅使用了三个数据集

1. Mip-NeRF 360 数据集
2. Tanks&Temples
3. Deep Blending

### Mip-NeRF 360 数据集

由 Google 在 CVPR 2022 上提出。

Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields

HomePage：[mip-NeRF 360](https://jonbarron.info/mipnerf360/)

该数据集由 9 个场景组成，其中 5 个室外场景和 4 个室内场景，每个场景都包含一个复杂的中心对象或具有详细背景的区域。

包含counter、room、bonsai、stump、kitchen、bicycle、garden、flowers、treehill

总共有大约16G数据

<img src="http://public.file.lvshuhuai.cn/images\image-20241120143450115.png" alt="image-20241120143450115" style="zoom:50%;" />

### Tanks&Temples

HomePage：[Tanks and Temples Benchmark](https://www.tanksandtemples.org/)

2017年

### ScanNet

ScanNet是一个大型真实RGB-D多模式数据集，包含超过250万张室内场景图片，1500个场景，带有相应的相机位姿、mesh模型、语义标签、实例标签和CAD模型。深度图以640×480分辨率拍摄，RGB图像以1296×968分辨率拍摄。3D模型由BundleFusion重建得到。

使用该数据集需要向作者申请，然后会收到一个python脚本，按照仓库中的README提示做就可以了。

## 评估指标

### 分类

#### 全参考质量评估指标、无参考质量评估指标和限参考质量评估指标

全参考质量评估指标（Full-Reference Quality Assessment Metrics）是指在图像或视频质量评价中，评估算法需要参考完整的原始图像或视频（即“无失真版本”）来衡量处理后图像或视频的质量。它们通过比较原始和处理后内容之间的差异，得出一个数值化的质量评分。

无参考质量评估指标（No-Reference Quality Assessment Metrics）是在图像或视频质量评估中，无需参考原始图像或视频的“无失真版本”，仅依赖于输入的退化图像或视频本身来预测其质量。这类指标在无法获取参考数据的实际应用中非常重要，例如实时视频传输、在线图像服务和监控系统。

限参考评估指标（Reduced-Reference Quality Assessment Metrics）只需要部分参考信息，例如统计特征，而不是完整的参考图像。

#### 按计算方式分

像素差异类、感知模型类、信息论类

### 峰值信噪比 PSNR（Peak Signal-to-Noise Ratio）

常用于衡量图像处理质量

通过比较原始图像（通常被认为是“真实值”）和处理后的图像（重建或退化后的图像）之间的相似度，评估图像的质量。

#### 公式

$$\text{PSNR}=10\cdot\log10(\frac{\text{MAX}_I^2}{\text{MSE}})$$

其中：

- $\text{MAX}_I$ 表示图像像素值的最大可能值（例如，对于 8 位图像为 255）。

- $\text{MSE}$（均方误差）表示原始图像和处理后图像之间的平均像素差平方，定义为：
    $\text{MSE} = \frac{1}{{m\cdot n}} \sum_{i=0}^{m-1} \sum_{j=0}^{n-1}[I(i,j)-K(i,j)]^2$

    其中：

    - $I(i,j)$​ 是原始图像的像素值，
    - $K(i, j)$ 是处理后图像的像素值，
    - $m\times n$是图像的总像素数。

#### 含义

1. **PSNR 的单位**：以分贝（dB）为单位。
2. **高 PSNR 值**：表示两幅图像之间的差异较小，即图像质量较好。
3. **低 PSNR 值**：表示两幅图像之间的差异较大，即图像质量较差。

### 结构相似性指数 SSIM（Structural Similarity Index Measure）

用于衡量两幅图像之间结构相似性的指标

与 PSNR 主要基于像素差异不同，SSIM 更加注重图像的感知质量，能够更好地反映人类视觉系统的感知特性。

#### 公式

$$\text{SSIM}(x, y) = \frac{{(2\mu_x\mu_y + C_1)(2\sigma_{xy} + C_2)}}{{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}}$$

其中：

- $x$ 和 $y$：表示两幅图像的对应窗口。
- $\mu_x,\mu_y$：分别是 $x$ 和 $y$ 的均值，反映亮度信息。
- $\sigma_x,\sigma_y$：分别是 $x$ 和 $y$ 的标准差，反映对比度信息。
- $\sigma_{xy}$：$x$ 和 $y$ 的协方差，反映结构信息。
- $C_1,C_2$：两个小常数，用于避免分母为零，通常取：
    $C_1=(K_1 L)^2,\quad C_2 = (K_2 L)^2$
    - $L$：表示像素值的动态范围（如 8 位图像时$L=255$）。
    - $K_1, K_2$：默认值通常分别为 0.01 和 0.03。

#### 分量解读

SSIM 将图像质量分解为三个主要分量：

1. **亮度比较（Luminance comparison）**

    $l(x, y) = \frac{{2\mu_x\mu_y + C_1}}{{\mu_x^2 + \mu_y^2 + C_1}}$

    衡量两图像亮度的相似程度。

2. **对比度比较（Contrast comparison）**

    $c(x, y) = \frac{{2\sigma_x\sigma_y + C_2}}{{\sigma_x^2 + \sigma_y^2 + C_2}}$

    衡量两图像对比度的相似程度。

3. **结构比较（Structure comparison）**

    $s(x, y) = \frac{{\sigma_{xy} + C_3}}{{\sigma_x\sigma_y + C_3}}, \quad C_3 = C_2 / 2$

    衡量两图像局部结构的相似程度。

SSIM 是这三部分的组合：

$\text{SSIM}(x, y) = [l(x, y)]^\alpha \cdot [c(x, y)]^\beta \cdot [s(x, y)]^\gamma$

一般情况下，$\alpha = \beta = \gamma = 1$

#### 范围

$[-1,1]$

- 1表示完全相同。
- 0表示没有相似性。
- 负值表示可能表示图像的某种失真。

### 学习感知图像块相似性 LPIPS（Learned Perceptual Image Patch Similarity）

用来衡量图像感知相似性的无参考质量评估指标

通过预训练的深度神经网络提取特征，评估图像块之间的感知差异，旨在更贴近人类视觉系统的感知特性

更关注感知层面的差异，对图像高频细节（如纹理、结构）的变化更加敏感

#### LPIPS 的核心思想

1. **深度特征提取**：使用预训练的卷积神经网络（如 VGG、AlexNet 等）从图像中提取深层次的特征。
2. **特征空间对比**：将两幅图像的特征图进行逐层比较，计算每一层特征之间的感知差异。
3. **感知加权**：通过学习的权重对每层特征差异进行加权，得到一个综合的感知相似度分数。

#### 公式

给定两幅图像 $x$ 和 $x_0$：

1. 使用预训练的神经网络提取每一层的特征表示： $F^l(x),F^l(x_0)$ 其中 $l$ 表示网络的第 $l$ 层。

2. 计算每一层特征图的欧几里得距离（L2 范数）：

    $d^l(x,x_0)=\|w^l\odot(F^l(x)-F^l(x_0))\|_2$

    - $w^l$：每层的感知加权系数（可通过训练优化）。
    - $\odot$：逐点乘法。

3. 对所有层的距离进行加权求和，得到最终的感知相似性分数：$\text{LPIPS}(x, x_0) = \sum_l d^l(x, x_0)$​

#### 含义

- **LPIPS 越小**：两幅图像的感知相似性越高。
- **LPIPS 越大**：两幅图像在感知上差异越明显。
