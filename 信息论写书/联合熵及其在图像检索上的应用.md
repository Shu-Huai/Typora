# 联合熵及其在图像检索上的应用

## 联合熵

### 联合熵的概念

熵这一概念的提出最初是用来衡量单个随机变量的不确定性。在考虑多个随机变量之间的相互关系以及它们组合起来所包含的不确定性时，我们需要联合熵的概念。例如，在通信系统中，发送端可能同时发送多个相关的信号（可以看作多个随机变量），为了准确地描述这些信号整体的不确定性，需要一个能够综合考虑它们联合概率分布的度量，联合熵就满足了这一需求。

对联合熵的研究有助于我们分析和处理复杂的信息系统，在数据压缩、通信、机器学习等众多领域有着广泛的应用。

为了更清晰的给出多个变量联合熵的定义，让我们先从具有两个随机变量的情况入手。

设$X$和$Y$为两个离散随机变量，$X$的取值范围为$\{x_1,x_2,\cdots,x_n\}$，$Y$的取值范围为$\{y_1,y_2,\cdots,y_m\}$，$(X,Y)$的联合概率分布为$p(x_i,y_j)$，其中$i=1,2,\cdots,n$，$y=1,2,\cdots,m$其联合熵定义为：
$$H(X,Y)=-\sum_{i = 1}^{n}\sum_{j = 1}^{m}p(x_i,y_j)\log p(x_i,y_j)$$

如果$p(x_i,y_j)=0$，则规定$p(x_i,y_j)\log p(x_i,y_j)=0$​。

从本质上讲，这个公式是在计算所有可能的取值组合$(x_i,y_j)$的概率$p(x_i,y_j)$与$\log p(x_i,y_j)$​乘积的负值的总和。对数函数在这里起到了量化概率所携带信息量的作用。概率越小，的值越大，意味着这个取值组合携带的信息量越大。

我们可以用一个简单的例子说明联合熵。

假设有两个随机变量$X$（表示天气情况，取值为“晴天”$x_1$、“雨天”$x_2$）和$Y$（表示是否带伞，取值为“带伞”$y_1$、“不带伞”$y_2$）。

假设它们的联合概率分布$p(x_i,y_j)$可能如下：

1. $p(x_1,y_1)=0.4$（晴天且带伞）
2. $p(x_1,y_2)=0.3$（晴天但不带伞）
3. $p(x_2,y_1)=0.2$（雨天且带伞）
4. $p(x_2,y_2)=0.1$（雨天但不带伞）

那么联合熵$H(X,Y)=-[p(x_1,y_1)\log p(x_1,y_1)+p(x_1,y_2)\log p(x_1,y_2)+p(x_2,y_1)\log p(x_2,y_1)+p(x_2,y_2)\log p(x_2,y_2)]$

计算可得$H(X,Y)=-[0.4\log 0.4 + 0.3\log 0.3+0.2\log 0.2 + 0.1\log 0.1]$。

这个值衡量了$X$（天气情况）和$Y$​（是否带伞）这两个随机变量组合在一起的不确定性。如果联合熵的值较大，说明这两个变量组合后的不确定性较高，我们较难根据一个变量的值去推断另一个变量的值；反之，如果联合熵较小，说明它们之间存在较强的关联，不确定性较低。

接下来，让我们将联合熵推广到具有多个随机变量的情况。

对于多个离散随机变量$X_1,X_2,\cdots,X_k$，联合熵$H(X_1,X_2,\cdots,X_k)=-\sum_{x_1}\sum_{x_2}\cdots\sum_{x_k}p(x_1,x_2,\cdots,x_k)\log p(x_1,x_2,\cdots,x_k)$。

多个随机变量联合熵考虑了所有$k$个随机变量的所有可能取值组合的联合概率，通过类似的方式来量化它们组合在一起的不确定性。例如，在一个更复杂的系统中，除了天气和带伞情况，还考虑是否穿雨衣$Z$等其他因素，就可以用多个随机变量的联合熵来衡量整体的不确定性。

### 联合熵的应用

联合熵理论在数据压缩、数据通信、机器学习等领域有许多应用。

在无损数据压缩算法中，联合熵可以用来确定压缩的理论极限。如果我们要压缩多个相关的数据序列，了解它们的联合熵可以帮助我们设计更有效的编码方案，使得压缩后的数据长度尽可能接近联合熵的值。

在通信领域中，当发送多个相关信息时，联合熵可以用于评估整个通信过程中的不确定性。通过分析联合熵和信道容量的关系，我们可以确定最优的传输策略，以保证信息的准确和高效传输。

在特征选择和模型评估等机器学习任务中，联合熵可以用来衡量不同特征之间的相关性。例如，通过计算特征之间的联合熵和互信息，可以选择最具信息量的特征子集，减少模型的复杂度和提高泛化能力。同时，在一些概率图模型中，联合熵的计算对于模型的推断和学习算法的设计也有着重要意义。

本章主要关注联合熵在图像检索机器学习算法中的应用。

## 联合熵在图像检索中的应用

### 对学习实例分布的优化

图像检索是计算机视觉的经典任务之一，是一种从大型数字图像数据库中浏览、搜索和检索图像的计算机技术，使用该技术能够从数十万幅图像中有效地检索出目标图像。研究者们关注提升检索的质量和效率。然而，通常情况下，高质量的检索需要大量学习实例的支持。大量的学习实例不仅在筛选的过程中需要大量的人力，在计算过程中也需要大量的计算资源。更重要的是，对于一些特殊的类别，很难获得大量的学习实例。因此，可以使用一种基于联合熵的学习模型，利用模型降低数据集中重复、无用甚至错误实例存在的可能性，优化学习实例的分布，以此来减少学习实例的数量。

具体而言，在对学习实例进行优化的过程中，使用基于联合熵的模型来评估学习实例分布的有效性。学习实例联合熵可以被视为每个学习实例的组合熵，公式如下：
$H(I)=\sum_{(n_1,n_2,\cdots)\in N}{p(n_1,n_2,\cdots,n_u,\cdots,n_n|I)\log(p(n_1,n_2,\cdots,n_u,\cdots,n_n|I))}$

其中，$p(n_1,n_2,\cdots,n_u,\cdots,n_n|I)$为考虑彼此上下文语义关系的学习实例共同存在的概率，$n_1,n_2,\cdots,n_u,\cdots,n_n$为不同的学习实例。算法的运行是通过逐步纳入新的学习实例进行的，在每一步中，除新纳入的学习实例$n_u$外，大部分学习实例的选择具有较高的确定性，而新实例$n_u$的选择则具有多种可能性。

由于直接计算联合熵的复杂度较高，我们可以采用近似方法。一种对联合熵的有效近似是使用一阶熵（First Order Entropy），在此情况下它是每个学习实例单独考虑的熵的总和：
$H_{FO}(I)=-\sum_{I_{j} \in I} \sum_{n_{j} \in N} p\left(n_{j} | I_{j}\right)\log \left(p\left(n_{j} | I_{j}\right)\right)$​

其中，$p(n_j|I_j)$为每个学习实例存在的概率。然而，这种近似方法忽略了不同学习实例之间的上下文语义关系，而上下文语义关系对熵的影响明显。针对该问题，考虑使用联合熵的二阶近似（Second Order Entropy），也即Bethe熵近似。

$H_{SO}(I)=\sum_{I_{j}, I_{k}} \sum_{\left(n_{1}, n_{2},...\right) \in N} -p\left(n_{j}, n_{k} | I_{j}, I_{k}\right) \log \left(p\left(n_{j}, n_{k} | I_{j}, I_{k}\right)\right)-(m - 1) H_{FO}$

其中，$m$为学习实例个数，$n$为学习实例，$I$为学习实例的外观特征，$p(n_j,n_k|I_j,I_k)$为图像的联合语义概率。

在算法的过程中，由于大多数学习实例已经存在，因此公式中的后一项是趋于稳定，而第一项是不断变化的，因为在合并过程中，联合熵会随着不同学习实例之间的上下文语义关系的变化而变化。

在实际计算中，不同学习实例之间的上下文语义关系很大程度上取决于基于不同特征描述符的相似性。在图像检索任务中，可以选取SIFT和GIST作为特征描述符。

### 特征描述符

SIFT（Scale-Invariant Feature Transform），即尺度不变特征变换，是一种计算机视觉领域中用于检测和描述图像局部特征的算法。该算法旨在从图像中提取出具有尺度不变性、旋转不变性以及对光照变化和图像变形具有一定鲁棒性的特征点，以此用于图像检索等计算机视觉任务。

算法的主要步骤如下

1. 尺度空间极值检测
   通过对原始图像进行不同尺度的高斯模糊，得到一系列图像，然后计算相邻尺度图像的差值，形成高斯差分金字塔。这样可以在不同尺度下检测到潜在的特征点。在高斯差分金字塔中，将每个像素与其周围的像素进行比较，寻找局部极值点。这些极值点可能是图像中的角点、边缘点等具有显著特征的点。
2. 关键点定位
   对检测到的极值点进行精确定位，去除一些不稳定的点。通过拟合三维二次函数来精确确定关键点的位置和尺度，同时计算关键点的主曲率，去除边缘响应较强的点，保留具有稳定特征的关键点。
3. 方向赋值
   为每个关键点确定一个主方向，使其具有旋转不变性。通过计算关键点周围邻域像素的梯度方向和幅值，统计梯度方向直方图，以直方图峰值对应的方向作为关键点的主方向。

SIFT算法不变性好，且能够提取出大量的特征点，为后续的匹配和识别提供了丰富的信息。

GIST是一种在计算机视觉领域用于描述图像整体特征的方法。它捕捉图像的全局语义信息，通过对图像的不同尺度和方向上的边缘响应进行统计和分析，得到一个能够代表图像整体特征的向量描述符。

算法的主要步骤如下

1. 边缘响应计算
   对原始图像进行多尺度的滤波操作，以获取不同尺度下的边缘信息。通常使用不同方向和尺度的滤波器（如Gabor滤波器）对图像进行卷积，得到多个边缘响应图。这些边缘响应图反映了图像在不同尺度和方向上的边缘强度和方向信息。
2. 空间分箱
   将图像划分成若干个空间区域（例如一个4×4的网格），对每个空间区域内的边缘响应进行统计。统计的信息可能包括边缘响应的平均值、方差等统计量，或者直接将边缘响应值累加。
3. 特征向量形成
   将各个空间区域统计得到的信息组合起来，形成一个固定维度的特征向量。这个特征向量就是GIST描述符，它代表了图像的整体特征。例如，如果将每个空间区域用一个包含多个统计量的向量表示，然后将所有区域的向量拼接起来，就可以得到一个较长的特征向量。

GIST方法全局描述性好，且计算相对简单，对图像的整体结构和纹理有较好的描述能力。

### 图像的联合语义概率

在图像检索任务中，SIFT和GIST可以有效提取图像的语义内容。如果两个学习实例基于SIFT描述符和GIST描述符彼此相似，则它们很可能在语义上相似。因此，可以将图像的联合语义概率定义为：
$p(n_j,n_k|I_j,I_k)\propto p(n_j,n_k|G)p(n_j,n_k|F)$

在图像检索任务中，使用的GIST描述符由5个尺度的6个定向边缘响应构建而成，聚集到4×4空间分辨率上，并使用基于欧氏距离的定义方式，从而提供最大的有效性。

$p\left(n_{j}, n_{k} | G\right)=\sqrt{\left(\sum_{i = 1}^{m}\left(n_{j}^{i}-n_{k}^{i}\right)^{2}\right)}$

在计算基于SIFT描述符的相似度时，可以使用词袋模型计算其基于SIFT的特征直方图，并将直方图用于评估不同图像之间的相似性，

特征向量由在图像上的规则网格上计算的SIFT特征组成，然后将这些计算得到的向量量化为视觉词。将每个视觉词的频率记录在空间平铺的每个图块的直方图中。而图像的最终特征向量是这些直方图的串联。

$p\left(n_{j}, n_{k} | F\right)=\sqrt{1-\sum_{i} \frac{\sqrt{H_{1}(i) * H_{2}(i)}}{\sum_{i} H_{1}(i) * \sum_{i} H_{2}(i)}}$​

因此，将上述公式归纳整理，我们可以得到对学习实例分布的优化方法。

在引入一个新的学习实例时，计算它与其他学习实例的联合语义概率，使得联合熵最大，即降低不同学习实例之间的相似性。在计算中，对于新引入的正学习实例，需要计算它与其他已知的正学习实例的联合语义概率；对于新引入的负学习实例，需要计算它与其他已知的负学习实例的联合语义概率。继续上述过程，直到学习实例数量足够多，能够用于模型训练。

实际的对比实验表明，使用基于联合熵的学习模型，为模型训练提供有限的学习实例（大约在100数量级），训练出的模型与其他使用大规模数据集训练的模型相比，在一些评估指标上能够得到没有显著差异的检索性能，在另一些指标上能够得到略逊于但可以接受的检索性能。