# Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation

## 基本信息

腾讯 Hunyuan3D-1.0：文本到3D和图像到3D生成的统一框架

一种从文本或图像生成3D模型对象的生成方法

腾讯开发

截至目前（2024.11.12）文章在arxiv上

开源了推理和demo，未开源训练代码

Github：[Tencent/Hunyuan3D-1](https://github.com/tencent/Hunyuan3D-1)

arxiv：[[2411.02293\] Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation](https://arxiv.org/abs/2411.02293)

## 摘要

虽然3D生成模型极大地改善了艺术家的工作流程，但现有的3D生成扩散模型存在生成速度慢、泛化能力差的问题。为了解决这个问题，我们提出了一种名为Hunyuan3D-1.0的两阶段方法，包括一个精简版和一个标准版，均支持文本和图像条件生成。在第一阶段，我们采用多视图扩散模型，该模型可在大约4秒内高效生成多视图RGB。这些多视图图像从不同视角捕捉3D资产的丰富细节，将任务从单视图放宽到多视图重建。在第二阶段，我们引入了一个前馈重建模型，该模型可在大约7秒内根据生成的多视图图像快速忠实地重建3D资产。重建网络学习处理多视图扩散引入的噪声和不一致性，并利用条件图像中的可用信息有效地恢复3D结构。我们的框架涉及文本到图像模型，即Hunyuan-DiT，使其成为支持文本和图像条件3D生成的统一框架。我们的标准版本的参数比我们的精简版和其他现有模型多3倍。我们的Hunyuan3D-1.0在速度和质量之间实现了令人印象深刻的平衡，显着缩短了生成时间，同时保持了所生成资产的质量和多样性。

## 研究背景

3D生成在多领域应用广泛，但高质量3D资产创建耗时，如何自动生成是研究目标。早期研究受限制，虽有大语言模型等成功经验，但 3D 资产的生成仍因数据等问题面临挑战。现有最大的3D数据集规模有限，一个可行的解决思路是利用2D生成模型的先验知识为3D生成提供特征。此前研究在3D生成方法上虽有探索，但存在诸多问题，如优化方法耗时、前馈方法泛化性差等。当前多视图生成和稀疏视图重建相关研究虽有进展，但缺乏整合框架来解决组合挑战，如多视图一致性和稀疏视图重建模型对输入图像类型的局限性等。

## 核心方法

### 贡献总结

- 引入了统一的框架Hunyuan3D-1.0，支持文本和图像条件下的3D生成。 
- 在多视图生成中设计了0高度姿势分布，最大化了生成的视图之间的可见区域。 
- 引入了一种视图感知的无分类器指导，平衡了不同视图生成的可控性和多样性。 
- 我们在稀疏视图重建过程中加入了涉及未校准条件图像的混合输入作为辅助视图，以补偿生成图像中看不见的部分

论文提出的方法总体上分为两个阶段

![image-20241112160447608](http://public.file.lvshuhuai.cn/images\image-20241112160447608.png)

### 多视角扩散模型

通过将多视图图像组织成网格来同时生成多视图图像。

利用Zero-1-to-3++方法，并用3倍大的模型替换Zero-1-to-3++中的模型，以此来扩大规模，增加泛化性。

使用参考注意力机制引导扩散模型生成与参考图像共享相似语义内容和纹理的图像。

扩散模型中使用自适应无分类器引导的采样技术，与传统CFG不同，自适应无分类器引导方法为不同的视图和时间步骤设置不同的CFG比例值，文章设置了一个较高的CFG比例，然后随着去噪过程的进行以及生成的图像的视图与条件图像的偏离而减小。

文章按照以下曲线设置正面视图 CFG 比例：

$$w_t=2+16\times(\frac{t}{1000})^2$$​

对于其他视图，使用此曲线的缩放版本：

$$w_{t,v}=w_t\times\tau_v$$

根据与正面的视距定义，$\tau\in[0.5,1]$

### 稀疏视图重建模型

- 基于 Transformer 的方法
- 使用多视图扩散模型生成的多视图图像在 2 秒内以前馈方式恢复 3D 形状
- 结合了校准和未校准的输入、轻量级超分辨率、显示 3D 表示
- 提出了一个用于三平面超分辨率的上采样模块
- 最终的输出是三维网格

## 实验

在腾讯内部的数据集上训练

在 64 个 A100 GPU 上完成。

### 实验效果

![图片](http://public.file.lvshuhuai.cn/图床\640.webp)

<img src="http://public.file.lvshuhuai.cn/images\image-20241112163305854.png" alt="image-20241112163305854" style="zoom:50%;" />

## 复现

开源了推理代码

在RTX 3090上进行了一些实验，推理能直接生成带纹理贴图着色信息的obj文件

<img src="http://public.file.lvshuhuai.cn/images\image-20241112164305446.png" alt="image-20241112164305446" style="zoom:50%;" />

## 问题

### 在处理有复杂纹理细节的对象上存在问题

prompt：一个神经网络

扩散模型的输出：

<img src="http://public.file.lvshuhuai.cn/images\image-20241112164043530.png" alt="image-20241112164043530" style="zoom:50%;" />

三维生成的输出：

<img src="http://public.file.lvshuhuai.cn/images\image-20241112164130155.png" alt="image-20241112164130155" style="zoom:50%;" />

### 显存占用高

在单张2080Ti 11G上进行推理，使用`save_memory`选项，使用精简版Pipeline依旧CUDA OOM，官方给出的推荐配置的显存需求`>=22G`，在RTX 3090 24G上进行推理也只能使用精简、`--save_memory`