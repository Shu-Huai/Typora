# 数学建模

## 实用工具

1. mathpix

## 数据来源

1. 比赛给的
2. 知网、谷歌学术
3. 国家统计局
4. awesome public datasets
5. epsdata
6. kaggle

## 数据预处理

### 缺失值

#### 均值、众数

- 适用赛题：人口的数量年龄、经济产业情况等统计数据，对个体精度要求不大的数据

#### 牛顿插值法

- 根据固定公式，构造近似函数，补上缺失值，普遍适用性强
- 缺点：区间边缘处的不稳定震荡，即龙格现象。不适合对导数有要求的题目
- 适用：热力学温度、地形测量、定位等只追求函数值精确而不关心变化的数据

#### 样条插值法

- 用分段光滑的曲线去插值，光滑意味着曲线不仅连续，还要有连续的曲率
- 适用赛题：零件加工，水库水流量，图像“基线漂移”，机器人轨迹等精度要求高、没有突变的数据

### 异常值

#### 正态分布$3\sigma$原则

求解步骤

1. 计算均值从和标准差
2. 判断每个数据值是否在$(\mu-3\sigma,\mu+3\sigma)$内，不在则为异常值

适用：总体符合正态分布

### 箱型图

下四分位数$Q_1$

上四分位数$Q_3$

四分位距$IQR=Q_3-Q_1$

设$[Q_1-1.5\times IQR,Q_3+1.5\times IQR]$为正常值

## 模型

### 层次分析法

#### 模型原理

应用AHP分析决策问题时，首先要把问题条理化、层次化，构造出一个有层次的结构模型。在这个模型下，复杂问题被分解为元素的组成部分。这些元素又按其属性及关系形成若干层次。上一层次的元素作为准则对下一层次有关元素起支配作用。

![image-20240830162157938](http://public.file.lvshuhuai.cn/images\image-20240830162157938.png)

这些层次可以分为三类

- 最高层
  这一层次中只有一个元素，一般它是分析问题的预定目标或理想结果，因此也称为目标层。

- 中间层
  这一层次中包含了为实现目标所涉及的中间环节，它可以由若干个层次组成，包括所需考虑的准则、子准则，因此也称为准则层。
- 最底层
  这一层次包括了为实现目标可供选择的各种措施、决策方案等，因此也称为措施层或方案层。

#### 基本步骤

1. 建立递阶层次结构模型

2. 构造出各层次中的所有判断矩阵
   对指标的重要性进行两两比较，构造判断矩阵，从而科学求出权重，元素$a_{ij}$是第$i$个指标相对于第$j$个指标的重要程度
   
   | 标度       | 含义                                           |
   | ---------- | ---------------------------------------------- |
   | 1          | 表示两个因素相比，具有同样重要性               |
   | 3          | 表示两个因素相比，一个因素比另一个因素稍微重要 |
   | 5          | 表示两个因素相比，一个因素比另一个因素明显重要 |
   | 7          | 表示两个因素相比，一个因素比另一个因素强烈重要 |
   | 9          | 表示两个因素相比，一个因素比另一个因素极端重要 |
   | 2，4，6，8 | 上述两相邻判断的中值                           |
   
   依次对变量进行两两比较，得到完整的判断矩阵
   两两比较的过程中忽略了其他因素，导致最后的结果可能出现矛盾
   所以需要一致性检验
   
3. 一致性检验
   一致矩阵
   正互反矩阵$A$为一致矩阵时当且仅当最大特征值$\lambda_{max}=n$。且当正互反矩阵A非一致时，一定满足$\lambda_{max}=n$，判断矩阵越不一致时，最大特征值与$n$相差就越大。
   步骤

   1. 计算一致性指标$CI=\frac{\lambda_{max}-n}{n-1}$

   2. 查找对应的平均随机一致性指标$RI$

      | n    | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   |
      | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
      | RI   | 0    | 0    | 0.52 | 0.89 | 1.12 | 1.26 | 1.36 | 1.41 | 1.46 | 1.49 |

      如果超过10，建立二级指标体系

   3. 计算一致性比例$CR=\frac{CI}{RI}$
      当$CR<0.1$，判断矩阵一致

4. 求权重后进行评价

   - 算术平均法
     1. 将判断矩阵按照列归一化（每一个元素除以其所在列的和）
     2. 将归一化的各列相加（按行求和）
     3. 将相加后得到的向量中每个元素除以$n$即可得到权重向量
   - 几何平均法
   - 特征值法
     1. 求出矩阵$A$的最大特征值以及其对应的特征向量
     2. 对求出的特征向量进行归一化即可得到我们的权重

#### 示例代码

```python
eig_val, eig_vec = np.linalg.eig(A)  # eig_val是特征值， eig_vec是特征向量
Max_eig = max(eig_val) 
CI = (Max_eig - n) / (n-1)
RI = [0, 0.0001, 0.52, 0.89, 1.12, 1.26, 1.36, 1.41, 1.46, 1.49, 1.52, 1.54, 1.56, 1.58, 1.59]  
CR = CI / RI[n]
if CR < 0.10:
    print('因为CR<0.10，所以该判断矩阵A的一致性可以接受!')
else:
    print('注意：CR >= 0.10，因此该判断矩阵A需要进行修改!')
ASum = np.sum(A, axis=0)
Stand_A = A / ASum
ASumr = np.sum(Stand_A, axis=1)
weights = ASumr / n
```

### Topsis法

#### 基本概念

逼近理想解排序法、优劣解距离法

TOPSIS法是一种常用的综合评价方法，能充分利用原始数据的信息，其结果能精确地反映各评价方案之间的差距。

TOPSIS法引入了两个基本概念

- 理想解
  设想的最优的解（方案），它的各个属性值都达到各备选方案中的最好的值
- 负理想解
  设想的最劣的解（方案），它的各个属性值都达到各备选方案中的最坏的值。

方案排序的规则是把各备选方案与理想解和负理想解做比较，若其中有一个方案最接近理想解，而同时又远离负理想解，则该方案是备选方案中最好的方案。TOPSIS通过最接近理想解且最远离负理想解来确定最优选择。

#### 模型原理

TOPSIS法是一种理想目标相似性的顺序选优技术，在多目标决策分析中是一种非常有效的方法。它通过归一化后（去量纲化）的数据规范化矩阵，找出多个目标中最优目标和最劣目标（分别用理归想一解化和反理想解表示），分别计算各评价目标与理想解和反理想解的距离，获得各目标与理想解的贴近度，按理想解贴近度的大小排序，以此作为评价目标优劣的依据。贴近度取值在0～1之间，该值愈接近1，表示相应的评价目标越接近最优水平；反之，该值愈接近0，表示评价目标越接近最劣水平。

#### 基本步骤

- 将原始矩阵正向化
  将原始矩阵正向化，就是要将所有的指标类型统一转为极大型指标
- 正向矩阵标准化
  标准化的方法有很多种，其主要目的就是去除量纲的影响，保证不同评价指标在同一数量级，且数据大小排序不变。
- 计算得分并归一化
  $S_i=\frac{D^-_i}{D^+_i+D^-_i}$，其中$S_i$为得分，$D^+_i$为评价对象与最大值的距离，$D^-_i$为评价对象与最小值的距离

#### 原始矩阵正向化

| 指标名称             | 指标特点         | 例子                         | 公式                                                         |
| -------------------- | ---------------- | ---------------------------- | ------------------------------------------------------------ |
| 极大型（效益型）指标 | 越大（多）越好   | 颜值、成绩、GDP增速          | /                                                            |
| 极小型（成本型）指标 | 越小（少）越好   | 脾气、费用、坏品率、污染程度 | $\widetilde{x}=max-x$，$\widetilde{x}$为转化后指标，$max$为指标最大值，$x$为指标值 |
| 中间型指标           | 越接近某个值越好 | 身高、水质量评估时的PH值     | $\{x_i\}$是一组中间型序列，最优值是$x_{best}$<br />$M=max\{|x_i-x_{best}|\},\widetilde{x_i}=1-\frac{|x_i-x_{best}|}{M}$ |
| 区间型指标           | 落在某个区间最好 | 体重、体温                   | $\{x_i\}$是一组区间型序列，最佳区间为$[a,b]$，正向化公式如下<br />$M=max\{a-min\{x_i\},max\{x_i\}-b\},\widetilde{x_i}=\left\{\begin{array}\\1-\frac{a-x_i}{M},x_i<a\\1,a\le x_i\le b\\1-\frac{x_i-b}{M},x_i>b\end{array} \right.$ |

#### 正向矩阵标准化

标准化的目的是消除不同指标量纲的影响

对每一个元素，除以其所在列的元素平方和的根式

#### 计算得分并归一化

1. 对每一列（每一个特征）求最大值
2. 对每一列求最小值
3. 对每一行（每一个评价对象）求与最大值的距离
4. 对每一行求与最小值的距离
5. 对每一行计算出未归一化的得分
6. 将得分归一化

#### 示例代码

```python
import numpy as np

n = 3
m = 4
kind = ["1", "2", "3", "4"]
A = np.array([[9, 10, 175, 120], [8, 7, 164, 80], [6, 3, 157, 90]])

bestA = 165
lowA = 90
highA = 100


def max_to_max(x):
    return x


def min_to_max(x):
    return max(x) - x


def mid_to_max(x, best):
    abss = abs(x - best)
    return 1 - abss / max(abss)


def reg_to_max(low, high, x):
    m = max(low - min(x), max(x) - high)
    if m == 0:
        m = 1
    result = np.zeros(x.shape[0])
    for i in range(x.shape[0]):
        if x[i] < low:
            result[i] = (1 - (low - x[i]) / m)
        elif x[i] > high:
            result[i] = (1 - (x[i] - high) / m)  # 如果指标值大于上限，则计算其与上限的距离比例
        else:
            result[i] = 1
    return result


def to_max():
    X = np.zeros(shape=(n, m))
    for i in range(m):
        if kind[i] == "1":
            X[:, i] = max_to_max(A[:, i])
        elif kind[i] == "2":
            X[:, i] = min_to_max(A[:, i])
        elif kind[i] == "3":
            X[:, i] = mid_to_max(A[:, i], bestA)
        else:
            X[:, i] = reg_to_max(lowA, highA, A[:, i])
    return X


X = to_max()


def normalize():
    for i in range(m):
        X[:, i] = X[:, i] / np.sqrt(sum(X[:, i] ** 2))


normalize()

x_max = np.max(X, axis=0)  # 计算标准化矩阵每列的最大值
x_min = np.min(X, axis=0)  # 计算标准化矩阵每列的最小值
d_z = np.sqrt(np.sum(np.square(X - np.tile(x_max, (n, 1))), axis=1))  # 计算每个参评对象与最优情况的距离d+
d_f = np.sqrt(np.sum(np.square(X - np.tile(x_min, (n, 1))), axis=1))  # 计算每个参评对象与最劣情况的距离d-
print('每个指标的最大值:', x_max)
print('每个指标的最小值:', x_min)
print('d+向量:', d_z)
print('d-向量:', d_f)

s = d_f / (d_z + d_f)  # 根据d+和d-计算得分s，其中s接近于1则表示较优，接近于0则表示较劣
Score = 100 * s / sum(s)  # 将得分s转换为百分制，便于比较
for i in range(len(Score)):
    print(f"第{i + 1}个标准化后百分制得分为：{Score[i]}")  # 打印每个参评对象的得分
```

### 熵权法

#### 基本概念

熵权法，物理学名词，按照信息论基本原理的解释，信息是系统有序程度的一个度量，熵是系统无序程度的一个度量；根据信息熵的定义，对于某项指标，可以用熵值来判断某个指标的离散程度，其信息熵值越小，指标的离散程度越大，该指标对综合评价的影响（即权重）就越大，如果某项指标的值全部相等，则该指标在综合评价中不起作用。因此，可利用信息熵这个工具，计算出各个指标的权重，为多指标综合评价提供依据。

熵权法是一种客观的赋权方法，它可以靠数据本身得出权重。

依据的原理：指标的变异程度越小，所反映的信息量也越少，其对应的权值也应该越低。

#### 基本步骤

1. 数据正向化
2. 数据标准化
   对标准化的矩阵记为$$z$$，则$$z_{ij}=\frac{x_{ij}}{\sqrt{\sum_{i=1}^{n}{x^2_{ij}}}}$$，如果$$x_{ij}$$存在负数，则标准矩阵$$\widetilde{z}=\frac{x-\min{\{x_{1j},\cdots,x_{nj}\}}}{\max{\{x_{1j},\cdots,x_{nj}\}-\min{\{x_{1j},\cdots,x_{nj}\}}}}$$
3. 计算概率矩阵$$p$$
   计算第$$j$$项指标下第$$i$$个样本所占的比重$$p_{ij}=\frac{\widetilde{z_{ij}}}{\sum_{i=1}^{n}{\widetilde{z_{ij}}}}$$
4. 计算熵权$$e_j=-\frac{1}{\ln n}\sum_{i=1}^{n}p_{ij}\ln{(p_{ij})}(j=1,\cdots,m),d_j=1-e_j,w_j=\frac{d_j}{\sum_{j=1}^{m}{d_j}}$$

#### 示例代码

```python
import numpy as np

X = np.array([[9, 0, 0, 0], [8, 3, 0.9, 0.5], [6, 7, 0.2, 1]])
Z = X / np.sqrt(np.sum(X * X, axis=0))


def mylog(p):
    n = len(p)  # 获取输入向量p的长度
    lnp = np.zeros(n)  # 创建一个长度为n，元素都为0的新数组lnp
    for i in range(n):  # 对向量p的每一个元素进行循环
        if p[i] == 0:  # 如果当前元素的值为0
            lnp[i] = 0  # 则在lnp中对应位置也设置为0，因为log(0)是未定义的，这里我们规定为0
        else:
            lnp[i] = np.log(p[i])  # 如果p[i]不为0，则计算其自然对数并赋值给lnp的对应位置
    return lnp  # 返回计算后的对数数组


n, m = Z.shape  # 获取标准化矩阵Z的行数和列数
D = np.zeros(m)  # 初始化一个长度为m的数组D，用于保存每个指标的信息效用值
for i in range(m):  # 遍历Z的每一列
    x = Z[:, i]  # 获取Z的第i列，即第i个指标的所有数据
    p = x / np.sum(x)  # 对第i个指标的数据进行归一化处理，得到概率分布p
    # 使用自定义的mylog函数计算p的对数。需要注意的是，如果p中含有0，直接使用np.log会得到-inf，这里使用自定义函数避免这个问题
    e = -np.sum(p * mylog(p)) / np.log(n)  # 根据熵的定义计算第i个指标的信息熵e
    D[i] = 1 - e  # 根据信息效用值的定义计算D[i]
W = D / np.sum(D) 
```

### 模糊综合评价模型

#### 概述

模糊是指客观事物差异的中间过渡中的“分明性”或“亦此亦被性”。如高个子与矮个子、年轻人与老年人、热水与凉水、环境污染严重与不严重等。在决策中，也有这种模糊的现象，如选举一个好干部，但怎样才算一个好干部？好干部与不好干部之间没有绝对分明和固定不变的界限。这些现象很难用经典的数学来描述。

#### 模型原理

#### 隶属函数确定方法

- 模糊统计法
- 借助已有的客观尺度
- 指派法

#### 评价问题概述

- 模糊评价问题是要把论域中的对象对应评语集中一个指定的评语或者将方案作为评语集并选择一个最优的方案。
- 在模糊综合评价中，引入三个集合
  因素集$$U=\{u_1,u_2,\cdots,u_n\}$$
  评语集$$V=\{v_1,v_2,\cdots,v_n\}$$
  权重集$$A=\{a_1,a_2,\cdots,a_n\}$$

#### 一级模糊综合评价模型

1. 确定因素集
2. 确定评语集
3. 确定各因素的权重
4. 确定模糊综合判断矩阵
5. 模糊综合评判，进行矩阵合成运算
   权重向量×判断矩阵

#### 多级模糊综合评价模型

1. 给出被评价的对象集合
2. 确定因素集
   如果因素众多，分成若干个子集
3. 确定评语集
4. 得到评价矩阵
5. 对每一个因素子集，分别作出综合决策
6. 将评价后的结果继续进行模糊综合评价

### 灰色关联分析算法

#### 

### 回归分析

#### 概述

统计学中，回归分析（regression analysis）指的是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。回归分析按照涉及的变量的多少，分为一元回归和多元回归分析；按照因变量的多少，可分为简单回归分析和多重回归分析；按照自变量和因变量之间的关系类型，可分为线性回归分析和非线性回归分析。

在大数据分析中，回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（预测器）之间的关系。这种技术通常用于预测分析以及发现变量之间的因果关系。例如，司机的鲁莽驾驶与道路交通事故数量之间的关系，最好的研究方法就是回归。

- 相关分析：研究两个或两个以上的变量之间相关程度及大小的一种统计方法
- 回归分析：寻找存在相关关系的变量间的数学表达式，并进行统计推断的一种统计方法
- 前面也说了，回归分析有两种分类方式
  - 根据变量的数目可以分为一元回归、多元回归
  - 根据自变量与因变量的表现形式，分为线性和非线性
- 所以，回归分析包括四个方向：一元线性回归分析、多元线性回归分析、一元非线性回归分析、多元非线性回归分析

#### 一般步骤

1. 确定回归方程中的解释变量和被解释变量
2. 确定回归模型建立回归方程
3. 对回归方程进行各种检验
4. 利用回归方程进行预测

#### 基本概念

- 因变量
  被预测或被解释的变量，用$$y$$表示
- 自变量
  预测或解释因变量的一个或者多个变量，用$$x$$表示
- 对于只有线性关系的两个变量，可以用一个方程来表示它们之间的线性关系
- 描述因变量$$y$$如何依赖于自变量$$x$$和误差项$$\epsilon$$的方程称为回归模型

## 机器学习

### 线性回归模型

#### 原理

使用数据点来寻找最佳拟合线。

公式，$$y=mx+c$$，其中$$y$$是因变量，$$x$$是自变量，利用给定的数据集求$$m$$和$$c$$的值。

#### 示例代码

```python
model = linear_model.LinearRegression()  #线性回归模型
model.fit(x_train, y_train)  #用x和y的数据拟合线性回归模型

print("系数为：", model.coef_)  #输出线性回归模型的系数
print("截距为：", model.intercept_)  #输出线性回归模型的截距
x_test = np.array([[4], [5], [6]])  #定义测试集的x值
y_predict = model.predict(x_test)  #预测测试集对应的y值
```

### 逻辑回归模型

#### 原理

通过将线性回归模型的输出通过一个逻辑函数进行转换，将连续的预测值映射到和之间，表示属于某一类的概率。逻辑回归的模型假设因变量服从伯努利分布，通过最大似然估计来拟合模型参数。

#### 示例代码

```python
model = LogisticRegression()  #拟合模型
model.fit(x, y) 
#输出模型参数
print('斜率：', model.coef_)
print('截距：', model.intercept_) 
#进行预测
X_new = np.array([[6],[7]])  #新的自变量
y_pred =model.predict（X_new） #预测分类结果
print('预测结果：', y_pred)
```

### 决策树模型

#### 原理

决策树通过对数据集进行递归地划分，以建立一个树形结构，每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一个类别标签或者一个连续值。在分类问题中，决策树的目标是创建一个能够对实例进行准确分类的模型。在建立决策树的过程中，通常使用信息增益（ID3算法）、基尼不纯度（CART算法）或者增益率等指标来选择最佳划分属性。

#### 示例代码

```py
iris = datasets.load_iris()
X = iris.data
y = iris.target
#划分训练装和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#建立决策树模型
model = DecisionTreeClassifier()
#拟合模型
model.fit(x_train, y_train)
#进行预测
y_pred = model.predict(X_test)
#输出预测准确率
print('预测准确率:', accuracy_score(y_test, y_pred))
```

### 朴素贝叶斯模型

#### 原理

朴素贝叶斯模型基于贝叶斯定理，它假设特征之间相互独立，即给定类别下特征之间是条件独立的。基于这个假设，可以利用训练数据计算出每个特征在各个类别下的条件概率，并结合贝叶斯定理计算出给定特征条件下各个类别的概率，最终选择概率最大的类别作为预测结果。

#### 示例代码

```python
model = GaussianNB()
# 拟合模型
model.fit(X_train, y_train)
# 进行预测
y_pred = model.predict(X_test)
```

### 随机森林模型

#### 原理

随机森林由多棵决策树组成，每棵树都是使用随机选择的特征和随机选择的样本进行训练。在进行预测时，随机森林对所有树的预测结果进行平均或多数投票，以得到最终的预测结果。通过引入随机性，随机森林能够降低模型的方差，提高模型的鲁棒性。

#### 示例代码

```python
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
#建立随机森林模型
model = RandomForestClassifier(n_estimators=100, random_state=42)  #这里指定了100碟树
# #拟合模型
model.fit(X_train, y_train)
#进行预测
y_pred = model.predict(X_test)
```

### 支持向量机

#### 原理

支持向量机的目标是找到一个能够将不同类别的数据分隔开的超平面，使得两个类别的间隔尽可能地大。在实际问题中，如果数据线性不可分，支持向量机会使用核函数将数据映射到高维空间中，以找到一个能够分隔数据的超平面。在分类问题中，支持向量机会选择支持向量（离超平面最近的数据点），并通过对支持向量进行处理来构建分类模型。

#### 示例代码

```python
#划分训练集和测试集
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
#建立支持向量机模型
model = SVC(kernel='rbf', C=1.0, gamma='scale')  # 这里使用了径向基函数核
#拟合模型
model.fit(x_train, y_train)
#进行预测
y_pred = model.predict(x_test)
```

### KNN模型

#### 原理

KNN算法的核心思想是通过计算待预测样本与训练集中各个样本的距离，然后选取距离最近的$$K$$个样本，根据这$$K$$个样本的类别进行投票决定待预测样本的类别。

#### 示例代码

```python
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
# 建立KNN模型
model = KNeighborsClassifier(n_neighbors=3)  # 这里指定了K=3
#拟合模型
model.fit(X_train, y_train)
#进行预测
y_pred = model.predict(X_test)
```

#### K-Means模型

#### 原理

通过迭代的方式将数据点分配到$$K$$个簇中，使得每个数据点到其所属簇的中心点（质心）的距离最小化。把$$n$$个点（可以是样本的一次观察或一个实例）划分到k个集群（cluster），使得每个点都属于离他最近的均值（即聚类中心，centroid）对应的集群。重复上述过程一直持续到重心不改变。

#### 示例代码

```python
#建立K均值模型
kmeans = KMeans(n_clusters=4)
#拟合模型
kmeans.fit(x)
#可视化结果
plt.scatter(x[:, 0], x[:, 1], c=kmeans.labels_, cmap='viridis')
centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
plt.show()
```

### Apriori算法

#### 原理

Apriori算法的核心思想是基于频繁项集的性质来发现数据集中的频繁项集。该算法使用了一种叫做Apriori原理的性质，即如果一个项集是频繁的，那么它的所有子集也必定是频繁的。Apriori算法通过迭代的方式生成候选项集，并使用逐层扫描数据集来逐步发现频繁项集。

#### 示例代码

```python
#将数据集进行独热编码
df_encoded = pd.get_dummies(df.drop（'ID', axis = 1)
#使用Apriori算法找出频繁项集
frequent_itemsets = apriori(df_encoded, min_support=0.6, use_colnames=True)
#找出关联规则
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.7)
```

### 机器学习建模流程

1. 确定问题：首先要明确竞赛题目中需要解决的问题，明确问题的类型（分类、回归、聚类等），以及需要预测的变量或目标。
2. 数据理解：初步的探索性分析，包括了解数据的基本统计特征、缺失值处理、异常值处理等。
3. 特征工程：对原始数据进行特征提取、选择和转换，包括特征缩放、特征组合、特征选择、处理类别型特征等。
4. 模型选择：根据问题的类型和数据的特点，选择适当的机器学习模型，比如决策树、随机森林、支持向量机、神经网络等。
5. 模型训练：使用训练集对选定的模型进行训练，通过优化模型参数来使模型拟合训练数据。
6. 模型评估：使用测试集对训练好的模型进行评估，评估模型的性能指标，比如准确率、召回率、F值、均方误差等。
7. 模型调优：根据评估结果对模型进行调优，可能包括调整超参数、交叉验证等。
8. 结果解释与报告：对模型的结果进行解释，分析模型对问题的贡献，撰写报告，呈现模型的应用效果和结论。

